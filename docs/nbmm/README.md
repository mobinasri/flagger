## Fitting A Negative Binomial Mixture Model to Coverage Counts Data

### Overview
A Negative Binomial Mixture Model (NBMM) can be used to extract the hidden components in a coverage counts data. `fit_nbmm.py` is a python script developed for this 
aim. It takes the coverage counts data as an input and uses Expectation Maximization (EM) algorithm to fit the mixture components to the coverage distribution.
It has four main components;

- Erroneous
- Falsely Duplicated
- Haploid
- Collapsed

Each of the first three components are modeled by a single NB component. The collapsed components is modeled by a series of NB distributions (subcomponents).
The means and variances of the falsely duplicated and haploid components and also the collapsed subcomponents are tied together.
Mean and variance of the falsely duplicated component is half of the haploid component. The mean and variance of the collapsed subcomponents are also tied 
to the haploid component by integer factors greater than 1.
The Erroneous component is modeded by a NB component whose parameters are independent from all the other components.

### Docker
The script is available in the docker image `mobinasri/flagger:v0.2`. It is recommended to use this image for running the program.


## Run program on real data
If we have a coverage counts data it should be saved in a tab delimited file with this format to be readable by the program:
```
0	40
1	37
2	11
3	15
4	19
5	43
6	80
7	142
8	217
9	349
10	450
11	509
12	644
13	703
14	699
15	720
```
The first column has the coverage value and the second column has the related frequency.
You can fit the model with this command:
```
docker run \
 -v ${INPUT_DIR}:${INPUT_DIR} \
 -v ${OUTPUT_DIR}:${OUTPUT_DIR} \
 mobinasri/flagger:v0.2 \
 python3 fit_mixture_nb.py \
 --include-dup-comp \
 --counts ${INPUT_DIR}/coverage.counts \
 --info "Some title text" \
 --out-prefix "${OUTPUT_DIR}/${ANALYSIS_NAME}"
```
`${INPUT_DIR}/coverage.counts` is the tab-delimited file shown above. The program has an option to include falsely duplicated component `--include-dup-comp` otherwise it does not consider this component.
It will create the output directory `${OUTPUT_DIR}/${ANALYSIS_NAME}` and put all the output tables and figures in this directory.
Here is the list of output files: 
- `${ANALYSIS_NAME}.coverage_component_probs.tsv`: It is a tab-delimited table containing the probability of each coverage value being generated by each of the four (or three) components.
- `${ANALYSIS_NAME}.loglikelihood.tsv`: It is a tab-delimited table showing how log-likelihood, AIC and BIC values have changed over EM iterations.
- `${ANALYSIS_NAME}.fit_model.pdf`: It is a figure showing how the whole model is fit to the given data.
- `${ANALYSIS_NAME}.loglikelihood.pdf`: It is a figure showing the components of the model after fitting to the given data.

## Run program on simulated data

It is also possible to simulate data and fit model to the simulated data. To do this the option `--simulate` should be enabled. To specify the generator model
all the parameters that start with `--simulate-` can be used:
```
  --simulate-max-ploidy SIMULATE_MAX_PLOIDY
                        Maximum ploidy in simulated data (should be greater
                        than 1)
  --simulate-include-dup-comp
                        The simulation will generate observations for false
                        duplication. The frequency of each component is
                        determined based on the given weights --simulate-
                        weights)
  --simulate-params SIMULATE_PARAMS
                        A comma-separated string that contains the parameters
                        of model for generating the simulated data; ${mean-
                        err},${var-err},${mean-hap},${var-hap}. [Default =
                        "1,2,30,40"]
  --simulate-weights SIMULATE_WEIGHTS
                        A comma-separated string that contains the weights of
                        the generator components for the simulated data;
                        ${err-weight},${dup-weight},${hap-weight},${col-
                        weight-first},${col-weight-rest}. If --simulate-
                        include-dup-comp is turned off then ${dup-weight}
                        should be removed from the string. [Default =
                        "0.05,0.05,0.7,0.1,0.1"]
  --simulate-n-obs SIMULATE_N_OBS
                        Number of simulated observations [Default = 10000]
```

You can run the program in the simulation mode by an example command like below:
```
docker run \
 -v ${INPUT_DIR}:${INPUT_DIR} \
 -v ${OUTPUT_DIR}:${OUTPUT_DIR} \
 mobinasri/flagger:v0.2 \
 python3 fit_mixture_nb.py \
 --include-dup-comp \
 --simulate \
 --simulate-include-dup-comp \
 --simulate-max-ploidy 10 \
 --simulate-include-dup-comp \
 --simulate-params "1,2,30,40" \
 --simulate-weights "0.05,0.05,0.7,0.1,0.1" \
 --simulate-n-obs 10000 \
 --info "${ANALYSIS_NAME}" \
 --out-prefix "${OUTPUT_DIR}/${ANALYSIS_NAME}"
```

If `--simulate` is enabled then the program will simulate coverage counts data and fit the mixture model right after that. It will also save the simulated data in the output
directory; `${ANALYSIS_NAME}.counts`.
This file can be reused by the program and can be passed to the `--counts` option in a separate run (without `--simulate`).
